{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "<font size=\"1\">\n",
    "Supplementary code for \"Build a Large Language Model From Scratch\": <a href=\"https://www.manning.com/books/build-a-large-language-model-from-scratch\">https://www.manning.com/books/build-a-large-language-model-from-scratch</a> by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff",
   "metadata": {},
   "source": [
    "# Finetuning for Unisup Naming Notifcn Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "9495f150-9d79-4910-d6e7-6c0d9aae4a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.2\n",
      "numpy version: 1.26.2\n",
      "tiktoken version: 0.6.0\n",
      "torch version: 2.2.1+cu121\n",
      "tensorflow version: 2.16.1\n",
      "pandas version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"      # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
   "metadata": {
    "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
   },
   "source": [
    "## E.2 Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c64df-4431-4d27-834d-2bb38a01fc02",
   "metadata": {},
   "source": [
    "- This section repeats the code from chapter 6 to load and prepare the dataset\n",
    "- Instead of repeating this code, one could copy & paste the LoRA code from section E.3 at the end of the chapter 6 notebook\n",
    "- (The LoRA code was originally the last section of chapter 6 but was moved to the appendix due to the length of chapter 6)\n",
    "- In similar fashion, we could also apply LoRA to the models in chapter 7 for instruction finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "424e4423-f623-443c-ab9e-656f9e867559"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_scope import (\n",
    "    download_and_unzip,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "\n",
    "\n",
    "#url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "#zip_path = \"sms_spam_collection.zip\"\n",
    "#extracted_path = \"sms_spam_collection\"\n",
    "#data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "data_file_path = \"/teamspace/studios/this_studio/spam_notificn_classifier.tsv\"\n",
    "#download_and_unzip(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "59055873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>X/PVV/COAT4/TA871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>A/PVV/PASS/FZ18/IF911A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>X/PVV/COAT4/IF911B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>10\"NG-4-3009-C511 FUITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>X/PVV/ANOD/TA023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80526</th>\n",
       "      <td>No</td>\n",
       "      <td>Major Intrus.insp_TO-FA-31100-5FL.ARRAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80527</th>\n",
       "      <td>No</td>\n",
       "      <td>DAL/BUOY COMPART_INTRUSIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80528</th>\n",
       "      <td>No</td>\n",
       "      <td>DAL/BUOY COMPART_INTRUSIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80529</th>\n",
       "      <td>No</td>\n",
       "      <td>DAL/BUOY COMPART_INTRUSIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80530</th>\n",
       "      <td>No</td>\n",
       "      <td>GIR/41-PSV-4010-Révision</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80531 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                      Text\n",
       "0        No                         X/PVV/COAT4/TA871\n",
       "1        No                    A/PVV/PASS/FZ18/IF911A\n",
       "2        No                        X/PVV/COAT4/IF911B\n",
       "3        No                   10\"NG-4-3009-C511 FUITE\n",
       "4        No                          X/PVV/ANOD/TA023\n",
       "...     ...                                       ...\n",
       "80526    No  Major Intrus.insp_TO-FA-31100-5FL.ARRAST\n",
       "80527    No                DAL/BUOY COMPART_INTRUSIVE\n",
       "80528    No                DAL/BUOY COMPART_INTRUSIVE\n",
       "80529    No                DAL/BUOY COMPART_INTRUSIVE\n",
       "80530    No                  GIR/41-PSV-4010-Révision\n",
       "\n",
       "[80531 rows x 2 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad6ac1",
   "metadata": {},
   "source": [
    "- When we check the class distribution, we see that the data contains \"ham\" (i.e., not-SPAM) much more frequently than \"spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e23e07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "No     57855\n",
      "Yes    22676\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14806f26",
   "metadata": {},
   "source": [
    "- For simplicity, and because we prefer a small dataset for educational purposes anyway (it will make it possible to finetune the LLM faster), we subsample (undersample) the dataset so that it contains 747 instances from each class\n",
    "- (Next to undersampling, there are several other ways to deal with class balances, but they are out of the scope of a book on LLMs; you can find examples and more information in the [`imbalanced-learn` user guide](https://imbalanced-learn.org/stable/user_guide.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d28f5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "No     22676\n",
      "Yes    22676\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_balanced_dataset(df):\n",
    "\n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"Yes\"].shape[0]\n",
    "\n",
    "    # Randomly sample \"ham' instances to match the number of 'spam' instances\n",
    "    ham_subset = df[df[\"Label\"] == \"No\"].sample(num_spam, random_state=123)\n",
    "\n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"Yes\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c168f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"No\": 0, \"Yes\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train_naming.csv\", index=None)\n",
    "validation_df.to_csv(\"validation_naming.csv\", index=None)\n",
    "test_df.to_csv(\"test_naming.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a49c8960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "No     57855\n",
      "Yes    22676\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0b865569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "85953c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 262, 717, 2420, 3275]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\"This is the first text message\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fa2e7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "94a5001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704006e6",
   "metadata": {},
   "source": [
    "- We also pad the validation and test set to the longest training sequence\n",
    "- Note that validation and test set samples that are longer than the longest training example are being truncated via `encoded_text[:self.max_length]` in the `SpamDataset` code\n",
    "- This behavior is entirely optional, and it would also work well if we set `max_length=None` in both the validation and test set cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "634c76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21646a5",
   "metadata": {},
   "source": [
    "- Next, we use the dataset to instantiate the data loaders, which is similar to creating the data loaders in previous chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd39c3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/batch.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
    "outputId": "3266c410-4fdb-4a8c-a142-7f707e2525ab"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {},
   "source": [
    "- As a verification step, we iterate through the data loaders and check that the batches contain 8 training examples each, where each training example consists of 120 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {},
   "source": [
    "- Lastly, let's print the total number of batches in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "6934bbf2-9797-4fbe-d26b-1a246e18c2fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9aa4a-ffd2-4d9f-a835-cce1059fe604",
   "metadata": {},
   "source": [
    "## 6.4 Initializing a model with pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647d77d",
   "metadata": {},
   "source": [
    "- In this section, we initialize the pretrained model we worked with in the previous chapter\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-2.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ebdaf-810e-46a2-9ad9-e017a04051b1",
   "metadata": {},
   "source": [
    "- This section repeats the code from chapter 6 to load and prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 184kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.38MiB/s]\n",
      "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 222kiB/s]\n",
      "model.ckpt.data-00000-of-00001:   4%|▍         | 275M/6.23G [00:09<03:06, 32.0MiB/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.ckpt.data-00000-of-00001: 100%|██████████| 6.23G/6.23G [03:21<00:00, 30.9MiB/s]\n",
      "model.ckpt.index: 100%|██████████| 20.7k/20.7k [00:00<00:00, 582kiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 1.84M/1.84M [00:00<00:00, 5.30MiB/s]\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.20MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_scope import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-xl (1558M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252614cd-7ce6-4908-83e6-3761f519904e",
   "metadata": {},
   "source": [
    "- To ensure that the model was loaded corrected, let's double-check that it generates coherent text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you closer to the goal.\n",
      "\n",
      "The goal is to be the best you\n"
     ]
    }
   ],
   "source": [
    "from previous_scope import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "abc7ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'X/PVV/COAT4/TA871' Answer with 'yes' or 'no'.\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'X/PV\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'X/PVV/COAT4/TA871'\"\n",
    "    \" Answer with 'yes' or 'no'.\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4323db",
   "metadata": {},
   "source": [
    "- In this section, we are modifying the pretrained LLM to make it ready for classification finetuning\n",
    "- Let's take a look at the model architecture first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b0372c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 1600)\n",
      "  (pos_emb): Embedding(1024, 1600)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (12): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (13): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (14): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (15): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (16): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (17): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (18): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (19): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (20): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (21): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (22): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (23): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (24): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (25): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (26): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (27): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (28): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (29): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (30): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (31): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (32): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (33): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (34): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (35): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (36): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (37): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (38): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (39): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (40): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (41): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (42): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (43): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (44): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (45): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (46): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (47): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174b31b-1ab5-4115-b01c-245369da5af3",
   "metadata": {},
   "source": [
    "- Above, we can see the architecture we implemented in chapter 4 neatly laid out\n",
    "- The goal is to replace and finetune the output layer\n",
    "- To achieve this, we first freeze the model, meaning that we make all layers non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ac3c61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca65c0b",
   "metadata": {},
   "source": [
    "- Then, we replace the output layer (`model.out_head`), which originally maps the layer inputs to 50,257 dimensions (the size of the vocabulary)\n",
    "- Since we finetune the model for binary classification (predicting 2 classes, \"spam\" and \"ham\"), we can replace the output layer as shown below, which will be trainable by default\n",
    "- Note that we use `BASE_CONFIG[\"emb_dim\"]` (which is equal to 768 in the `\"gpt2-small (124M)\"` model) to keep the code below more general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393751a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/trainable.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "42690f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6bb41",
   "metadata": {},
   "source": [
    "- Technically, it's sufficient to only train the output layer\n",
    "- However, as I found in [experiments finetuning additional layers](https://magazine.sebastianraschka.com/p/finetuning-large-language-models) can noticeably improve the performance\n",
    "- So, we are also making the last transformer block and the final `LayerNorm` module connecting the last transformer block to the output layer trainable\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/trainable.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b9f3f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8aeb1",
   "metadata": {},
   "source": [
    "- We can still use this model similar to before in previous chapters\n",
    "- For example, let's feed it some text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e83bebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca93a74",
   "metadata": {},
   "source": [
    "- What's different compared to previous chapters is that it now has two output dimensions instead of 50,257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5f2d9b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[ 0.1746,  0.6253],\n",
      "         [-0.7584,  0.0307],\n",
      "         [ 0.1333,  0.7560],\n",
      "         [-0.0609,  0.4414]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ed93a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/input-and-output.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f51d8",
   "metadata": {},
   "source": [
    "- As discussed in previous chapters, for each input token, there's one output vector\n",
    "- Since we fed the model a text sample with 6 input tokens, the output consists of 6 2-dimensional output vectors above\n",
    "- In chapter 3, we discussed the attention mechanism, which connects each input token to each other input token\n",
    "- In chapter 3, we then also introduced the causal attention mask that is used in GPT-like models; this causal mask lets a current token only attend to the current and previous token positions\n",
    "- Based on this causal attention mechanism, the 6th (last) token above contains the most information among all tokens because it's the only token that includes information about all other tokens\n",
    "- Hence, we are particularly interested in this last token, which we will finetune for the spam classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a2920ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-0.0609,  0.4414]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a733423",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/attention-mask.webp\" width=200px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d21766",
   "metadata": {},
   "source": [
    "## 6.6 Calculating the classification loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295e484",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-3.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c7882",
   "metadata": {},
   "source": [
    "- Before we can start finetuning (/training), we first have to define the loss function we want to optimize during training\n",
    "- The goal is to maximize the spam classification accuracy of the model; however, classification accuracy is not a differentiable function\n",
    "- Hence, instead, we minimize the cross entropy loss as a proxy for maximizing the classification accuracy (you can learn more about this topic in lecture 8 of my freely available [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression) class.\n",
    "\n",
    "- Note that in chapter 5, we calculated the cross entropy loss for the next predicted token over the 50,257 token IDs in the vocabulary\n",
    "- Here, we calculate the cross entropy in a similar fashion; the only difference is that instead of 50,257 token IDs, we now have only two choices: spam (label 1) or ham (label 0).\n",
    "- In other words, the loss calculation training code is practically identical to the one in chapter 5, but we now only have two labels instead of 50,257 labels (token IDs).\n",
    "\n",
    "\n",
    "- Consequently, the `calc_loss_batch` function is the same here as in chapter 5, except that we are only interested in optimizing the last token `model(input_batch)[:, -1, :]` instead of all tokens `model(input_batch)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6c05ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e26d2",
   "metadata": {},
   "source": [
    "The `calc_loss_loader` is exactly the same as in chapter 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e255ce91-d73a-4854-90a4-95804928eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in chapter 5\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69053f",
   "metadata": {},
   "source": [
    "- Using the `calc_closs_loader`, we compute the initial training, validation, and test set losses before we start training\n",
    "- Here, we use `torch.no_grad()` so that no gradients are computed during the forward pass, which reduces memory consumption and speeds up computations since we are not training the model yet\n",
    "- Via the `device` setting, the  model automatically runs on a GPU if a GPU with Nvidia CUDA support is available and otherwise runs on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "88cad732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.709\n",
      "Validation loss: 0.690\n",
      "Test loss: 0.680\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382eaccb",
   "metadata": {},
   "source": [
    "- Similar to the `calc_loss_loader` function above, we can define a `calc_accuracy_loader` function that calculates the classification accuracy by checking how many predicted class (spam and ham) labels match the given labels in the dataset\n",
    "- Note that the classification accuracy is a mathematically non-differentiable function, and we only use it for evaluation; hence, we can disable the gradient calculation permanently to save resources here\n",
    "- We can disable the gradient tracking either using the `with torch.no_grad():` inside the function or by using the `@torch.no_grad()` function decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "02e6f057-1383-4ece-8444-0a88e71ac75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Disable gradient tracking for efficiency\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe",
   "metadata": {},
   "source": [
    "- Let's check the initial classification accuracy before we start training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7965358",
   "metadata": {},
   "source": [
    "- As we can see, the model only gets roughly half (50%) of the predictions correctly\n",
    "- In the next section, we train the model to improve the classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b455e",
   "metadata": {},
   "source": [
    "## 6.7 Finetuning the model on supervised data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab5d2a",
   "metadata": {},
   "source": [
    "- In this section, we define and use the training function to improve the classification accuracy of the model\n",
    "- The `train_classifier_simple` function below is practically the same as the `train_model_simple` function we used for pretraining the model in chapter 5\n",
    "- The only two differences are that we now \n",
    "  1. track the number of training examples seen (`examples_seen`) instead of the number of tokens seen\n",
    "  2. calculate the accuracy after each epoch instead of printing a sample text after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96abac2f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/training-loop.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "aeac146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall the same as `train_model_simple` in chapter 5\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous epoch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca708d8f",
   "metadata": {},
   "source": [
    "- The `evaluate_model` function used in the `train_classifier_simple` is the same as the one we used in chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5f78d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as chapter 5\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3785941",
   "metadata": {},
   "source": [
    "- The training takes about 5 minutes on a M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a793f6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.686, Val loss 0.685\n",
      "Ep 1 (Step 000050): Train loss 0.359, Val loss 0.378\n",
      "Ep 1 (Step 000100): Train loss 0.059, Val loss 0.082\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 2 (Step 000150): Train loss 0.058, Val loss 0.131\n",
      "Ep 2 (Step 000200): Train loss 0.008, Val loss 0.053\n",
      "Ep 2 (Step 000250): Train loss 0.035, Val loss 0.053\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 3 (Step 000300): Train loss 0.043, Val loss 0.179\n",
      "Ep 3 (Step 000350): Train loss 0.009, Val loss 0.063\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 4 (Step 000400): Train loss 0.012, Val loss 0.096\n",
      "Ep 4 (Step 000450): Train loss 0.028, Val loss 0.076\n",
      "Ep 4 (Step 000500): Train loss 0.030, Val loss 0.039\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.108, Val loss 0.042\n",
      "Ep 5 (Step 000600): Train loss 0.003, Val loss 0.056\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 3.84 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b9584",
   "metadata": {},
   "source": [
    "- Similar to chapter 5, we use matplotlib to plot the loss function for the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f96c1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e8576ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815d7db",
   "metadata": {},
   "source": [
    "- Above, based on the downward slope, we see that the model learns well\n",
    "- Furthermore, the fact that the training and validation loss are very close indicates that the model does not tend to overfit the training data\n",
    "- Similarly, we can plot the accuracy below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b2c3a4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8560dd8",
   "metadata": {},
   "source": [
    "- Based on the accuracy plot above, we can see that the model achieves a relatively high training and validation accuracy after epochs 4 and 5\n",
    "- However, we have to keep in mind that we specified `eval_iter=5` in the training function earlier, which means that we only estimated the training and validation set performances\n",
    "- We can compute the training, validation, and test set performances over the complete dataset as follows below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "82d28c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.81%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 97.67%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf19529",
   "metadata": {},
   "source": [
    "- We can see that the training and test set performances are practically identical\n",
    "- However, based on the slightly lower test set performance, we can see that the model overfits the training data to a very small degree\n",
    "- This is normal, however, and this gap could potentially be further reduced by increasing the model's dropout rate (`drop_rate`) or the `weight_decay` in the optimizer setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b",
   "metadata": {
    "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b"
   },
   "source": [
    "## 6.8 Using the LLM as a SPAM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4a82-61ef-4d0a-9858-8988e844f12c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-4.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d61064",
   "metadata": {},
   "source": [
    "- Finally, let's use the finetuned GPT model in action\n",
    "- The `classify_review` function below implements the data preprocessing steps similar to the `SpamDataset` we implemented earlier\n",
    "- Then, the function returns the predicted integer class label from the model and returns the corresponding class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "fe519d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "\n",
    "    # Truncate sequences if they too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the classified result\n",
    "    return \"Proper Naming Notfcn\" if predicted_label == 1 else \"Wrong Naming Notificn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e5a5e",
   "metadata": {},
   "source": [
    "- Let's try it out on a few examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "39a31ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Naming Notificn\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"A/LIFT/MECA/HU-JH-7891\"\n",
    "    \n",
    ")\n",
    "\n",
    "print(classify_review(text_1, model, tokenizer, device, max_length=train_dataset.max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "81bb63cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Naming Notificn\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"S/PIP/TBR1/PU3\"\n",
    ")\n",
    "\n",
    "print(classify_review(text_2, model, tokenizer, device, max_length=train_dataset.max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbecfa",
   "metadata": {},
   "source": [
    "- Finally, let's save the model in case we want to reuse the model later without having to train it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2ds9ywjMwvIW",
   "metadata": {
    "id": "2ds9ywjMwvIW"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe27c3",
   "metadata": {},
   "source": [
    "- Then, in a new session, we could load the model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "acff5713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\")\n",
    "model.load_state_dict(model_state_dict)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
